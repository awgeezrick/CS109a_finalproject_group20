{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Split\n",
    "Testing of the Spotify Recommender will be done by predicting missing songs in a playlist.\n",
    "\n",
    "A set of 10,000 test playlists are selected where each playlist has at least 50 tracks.\n",
    "\n",
    "From each of the test playlists, 10 songs will be withheld so that the test dataset contains two datasets; 1 for the 'given' tracks of a playlist, and another one for the 'withheld' tracks of a playlist.\n",
    "\n",
    "A train dataset is created which excludes all the test playlists.\n",
    "\n",
    "The goal of a recommender is to predict the 'withheld' tracks when the 'given' tracks are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import func #Table, Column, Integer, String, Float, MetaData, and_, or_, \n",
    "\n",
    "sys.path.append('../../')\n",
    "from spotify_api import get_spotify_data, get_tracks, get_artists, get_audiofeatures\n",
    "from spotify_database import get_session, display_time\n",
    "from spotify_utils import Table_Generator, List_Generator, pickle_load, pickle_save\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_playlists  = 10000\n",
    "num_withheld_tracks = 10\n",
    "min_playlist_length = 50\n",
    "\n",
    "if num_withheld_tracks>=min_playlist_length:\n",
    "    print(\"WARNING: Unpredictable results when the number of withheld tracks exceeds the minimum playlist length.\")\n",
    "\n",
    "data_path = '../../data/SpotifyDataSet'\n",
    "db_path = '../../data/SpotifyDataSet/spotify_songs.db'\n",
    "\n",
    "# Get sesion\n",
    "session = get_session(db_path)\n",
    "engine = create_engine('sqlite:///' + db_path)\n",
    "\n",
    "# Get Songs class\n",
    "Playlists = getattr(get_session, \"Playlists\")\n",
    "Artists = getattr(get_session, \"Artists\")\n",
    "Tracks = getattr(get_session, \"Tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Creation Process\n",
    "1. Get a list of all playlists and their length\n",
    "2. Define a list of test playlist ID's where each playlist has more than the required minimum playlist length\n",
    "3. Define a list of train playlist ID's that includes all playlists except this designated as the test list\n",
    "4. Split the test playlist into 'given' and a 'withheld' groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - get list of all playlists and their length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_counts_csv(data_path, filename:str=\"df_counts.csv\")->str:\n",
    "    \"\"\"\n",
    "    Create a CSV file that includes a list of all playlist ids from \n",
    "    the database and the count of tracks in each playlist.\n",
    "    If the given filename already exists, create a new file with \n",
    "    a number extension.\n",
    "    Returns the name of saved file without the path.\n",
    "    \"\"\"\n",
    "    # see if directory exists\n",
    "    if not os.path.isdir(data_path):\n",
    "        print (\"'{}' is not a directory.\".format(data_path))\n",
    "        print (\"Provide a valid directory path and rerun function.\")\n",
    "        return False\n",
    "    \n",
    "    # see if file already exists\n",
    "    if os.path.isfile(os.path.join(data_path, filename)):\n",
    "        f_split = filename.split(\".\") # split extension from filename\n",
    "        filename_stem = \"_\".join([f_split[-2],\"1\"]) # add a _1 to filename\n",
    "        f_split[-2] = filename_stem # replace the stem\n",
    "        filename = \".\".join(f_split) #rejoin\n",
    "    \n",
    "    print(\"Creating counts csv file: {}\".format(filename))\n",
    "    \n",
    "    # find all playlists with more than 50 tracks\n",
    "    # Count Number of tracks in playlists\n",
    "    songs_per_playlist = display_time(session.query(Playlists.playlist_id, \n",
    "                                    func.count(Playlists.track_name).label('count')).group_by(Playlists.playlist_id).all)\n",
    "\n",
    "    # Cast to dataframe\n",
    "    df_counts = pd.DataFrame(data=songs_per_playlist, \n",
    "                             columns=[\"playlist_id\",\"song_count\"]).set_index('playlist_id', drop=True)\n",
    "\n",
    "    df_counts.to_csv(os.path.join(data_path, filename))\n",
    "    \n",
    "    print(\"Saved csv file: {}\".format(os.path.join(data_path, filename)))\n",
    "    \n",
    "    return filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the counts csv file\n",
    "# filename = create_counts_csv(data_path, filename=\"df_counts.csv\")\n",
    "\n",
    "# read in file just saved\n",
    "df_counts = pd.read_csv(os.path.join(data_path, \"df_counts.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 and 3 - define train and test playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the playlists id's where the playlist has >=50 songs\n",
    "np.random.seed(1)\n",
    "playlistids_gt_minlen = df_counts[df_counts.song_count>=min_playlist_length].playlist_id.values\n",
    "test_playlistids  = np.random.choice(playlistids_gt_minlen, num_test_playlists, replace=False)\n",
    "train_playlistids = list(set(df_counts.playlist_id.values)) #- set(test_playlistids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to Execute: 1654.25 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get all playlists from the db - 30 minutes\n",
    "playlist_db = display_time(session.query(Playlists.playlist_id, Playlists.track_uri, Playlists.track_name, Playlists.artist_uri, Playlists.artist_name).all) \n",
    "\n",
    "# Cast to dataframe\n",
    "# takes 10 minutes\n",
    "df_playlists = pd.DataFrame(data=playlist_db, \n",
    "                         columns=[\"playlist_id\",\"track_uri\",\"track_name\",\"artist_uri\",\"artist_name\"])\n",
    "\n",
    "# takes 5 minutes - 8gb file\n",
    "df_playlists.to_csv(os.path.join(data_path, \"df_playlists.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataframe\n",
    "df_playlists_train = df_playlists[df_playlists.playlist_id.isin(train_playlistids)]\n",
    "\n",
    "# create test dataframe\n",
    "df_playlists_test = df_playlists[df_playlists.playlist_id.isin(test_playlistids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - split test set into 'given' and 'withheld' datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca425571b3574029a7cd7b9b78ba9c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split test into given and withheld', max=10000, style=Progresâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# break test dataframe into 'given' and 'withheld' groups\n",
    "# takes 20 minutes\n",
    "df_playlists_test_withheld = pd.DataFrame()\n",
    "df_playlists_test_given = pd.DataFrame()\n",
    "\n",
    "# iterate over selected test playlists\n",
    "# select random 10 tracks from each and put them in 'withheld' dataframe\n",
    "# keep remaining in a 'given' dataframe\n",
    "seed = np.random.RandomState(seed=2)\n",
    "\n",
    "test_playlist_ids = np.unique(df_playlists_test.playlist_id.values)\n",
    "for playlist_id in tqdm(test_playlist_ids, desc=\"split test into given and withheld\"):\n",
    "    plist = df_playlists_test[df_playlists_test.playlist_id==playlist_id]\n",
    "    \n",
    "    withheld_tracks = plist.sample(num_withheld_tracks, random_state=seed, replace=False, axis=0)\n",
    "    given_tracks_indexes = list(set(plist.index.values) - set(withheld_tracks.index.values))\n",
    "    given_tracks = plist[plist.index.isin(given_tracks_indexes)]\n",
    "    \n",
    "    # create entries in withheld and given df's\n",
    "    df_playlists_test_withheld = df_playlists_test_withheld.append(withheld_tracks)\n",
    "    df_playlists_test_given = df_playlists_test_given.append(given_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to CSV files\n",
    "df_playlists_test_withheld.to_csv(os.path.join(data_path, \"df_playlists_test_withheld.csv\"), index_label='index')\n",
    "df_playlists_test_given.to_csv(os.path.join(data_path, \"df_playlists_test_given.csv\"), index_label='index')\n",
    "df_playlists_train.to_csv(os.path.join(data_path, \"df_playlists_train.csv\"), index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To read in saved Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/cs109a/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# read respective CSV files as dataframes\n",
    "df_playlists_test_withheld = pd.read_csv(os.path.join(data_path, \"df_playlists_test_withheld.csv\"), index_col='index')\n",
    "df_playlists_test_given    = pd.read_csv(os.path.join(data_path, \"df_playlists_test_given.csv\"), index_col='index')\n",
    "df_playlists_train         = pd.read_csv(os.path.join(data_path, \"df_playlists_train.csv\"), index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
